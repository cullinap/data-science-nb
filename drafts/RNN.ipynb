{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are Recurrent Neural Networks (RNN)?\n",
    "\n",
    "* Trad NN's take fixed amount of data all in at once\n",
    "* RNNs consume inputs one at a time in sequence\n",
    "* At each step the RNN does a calculation before producing an output called the hidden state\n",
    "* The hidden state is then combined with the next input in the sequence that produces another output\n",
    "* The calculations from the prvious step form the hidden state aka context\n",
    "* RNNs can be adapted to take in many or one inputs and output many or one or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters\n",
      "{'o', 'a', 'e', 'y', 'f', 'n', 'h', 'm', 'v', 'g', 'r', 'i', 'w', 'd', 'c', ' ', 'u'}\n",
      "\n",
      "int2char\n",
      "{0: 'o', 1: 'a', 2: 'e', 3: 'y', 4: 'f', 5: 'n', 6: 'h', 7: 'm', 8: 'v', 9: 'g', 10: 'r', 11: 'i', 12: 'w', 13: 'd', 14: 'c', 15: ' ', 16: 'u'}\n",
      "\n",
      "characters to integers\n",
      "{'o': 0, 'a': 1, 'e': 2, 'y': 3, 'f': 4, 'n': 5, 'h': 6, 'm': 7, 'v': 8, 'g': 9, 'r': 10, 'i': 11, 'w': 12, 'd': 13, 'c': 14, ' ': 15, 'u': 16}\n"
     ]
    }
   ],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# join the sentences together and extract the unique characters from the combined sentence\n",
    "chars = set(''.join(text))\n",
    "print('unique characters')\n",
    "print(chars)\n",
    "print('')\n",
    "\n",
    "# create a dicitonary that maps integers to characters\n",
    "int2char = dict(enumerate(chars))\n",
    "print('int2char')\n",
    "print(int2char)\n",
    "print('')\n",
    "\n",
    "# create another dictionary that maps characters to integers\n",
    "char2int = {c: i for i, c in int2char.items()}\n",
    "print('characters to integers')\n",
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest string length in dataset\n",
      "15\n",
      "['hey how are you', 'good i am fine ', 'have a nice day']\n"
     ]
    }
   ],
   "source": [
    "# find length of the longest string in our data\n",
    "maxlen = len(max(text,key=len))\n",
    "print('longest string length in dataset')\n",
    "print(maxlen)\n",
    "\n",
    "# hey how goes it is our longest sentence so we pad the other two sentences with whitespace equiv to that sent\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen:\n",
    "        text[i] += ' '\n",
    "        \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey how are yo', 'good i am fine', 'have a nice da']\n",
      "['ey how are you', 'ood i am fine ', 'ave a nice day']\n"
     ]
    }
   ],
   "source": [
    "# create lists that will hold our input and target seq\n",
    "# remove last character of input seq and first character of target seq\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # remove last character from input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # remove first character from target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    \n",
    "print(input_seq)\n",
    "print(target_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2, 3, 15, 6, 0, 12, 15, 1, 10, 2, 15, 3, 0], [9, 0, 0, 13, 15, 11, 15, 1, 7, 15, 4, 11, 5, 2], [6, 1, 8, 2, 15, 1, 15, 5, 11, 14, 2, 15, 13, 1]]\n",
      "[[2, 3, 15, 6, 0, 12, 15, 1, 10, 2, 15, 3, 0, 16], [0, 0, 13, 15, 11, 15, 1, 7, 15, 4, 11, 5, 2, 15], [1, 8, 2, 15, 1, 15, 5, 11, 14, 2, 15, 13, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# convert to seq of ints by mapping to char2int so we can one-hot encode\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[char] for char in input_seq[i]]\n",
    "    target_seq[i] = [char2int[char] for char in target_seq[i]]\n",
    "    \n",
    "print(input_seq)\n",
    "print(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
     ]
    }
   ],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "# one hot encode\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # create a multi-dim array of zeros with desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # replace the 0 at the relevant character index with a 1 to rep that char\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "# input shape --> (batchsize, sequence length, one-hot encoding size)\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean true if a GPU is available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# if GPU available we'll set deviceto GPU\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is avaiable')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):  \n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # define some params\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # define the layers\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # initialize the hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # this method generates the first hidden state of zeros which we'll use in th eforward pass\n",
    "        # We'll send the tensor hoding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model with hyperparams\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also se thte model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyper params\n",
    "n_epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "# Define loss, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100......\n",
      "loss: 2.331434726715088\n",
      "Epoch: 20/100......\n",
      "loss: 2.006199359893799\n",
      "Epoch: 30/100......\n",
      "loss: 1.6283578872680664\n",
      "Epoch: 40/100......\n",
      "loss: 1.246556282043457\n",
      "Epoch: 50/100......\n",
      "loss: 0.910328209400177\n",
      "Epoch: 60/100......\n",
      "loss: 0.6482764482498169\n",
      "Epoch: 70/100......\n",
      "loss: 0.4624045193195343\n",
      "Epoch: 80/100......\n",
      "loss: 0.3413064777851105\n",
      "Epoch: 90/100......\n",
      "loss: 0.26413577795028687\n",
      "Epoch: 100/100......\n",
      "loss: 0.2127576768398285\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # clear existing grads from previous epoch\n",
    "    input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}/{n_epochs}......')\n",
    "        print(f'loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mode, character):\n",
    "    # one-hot encode our input to fit into the model\n",
    "    char = np.array([[char2int[c] for c in character]])\n",
    "    char = one_hot_encode(char, dict_size, char.shape[1], 1)\n",
    "    char = torch.from_numpy(char)\n",
    "    char.to(device)\n",
    "    \n",
    "    out, hidden = model(char)\n",
    "    \n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # take class with highest probability\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    \n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "    \n",
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval()\n",
    "    start = start.lower()\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "        \n",
    "    return ''.join(chars)\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good i am fine '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 15, 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
